\documentclass{article}

\usepackage{amsfonts, amsmath, amsthm}
\usepackage[letterpaper, total={6in, 9in}]{geometry}
% \usepackage[noend]{algorithmic}
\usepackage[vlined,ruled,linesnumbered]{algorithm2e}

%% preamble packages and symbols
\input{preamble_packages}
\input{preamble_symbols}

% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

%% it is often convenient to define shortcuts for some important notations
\input{shortcuts.tex}

\title{Language-Guided Whole-Body Humanoid Control via Conditional Trajectory Diffusion}

\author{Kevin H. Yang}

\begin{document}
\maketitle

%% Abstract
\begin{abstract}
We propose a language-conditioned whole-body control framework for humanoid robots.
Our approach aims to map natural language commands to dynamically stable, full-body control trajectories.
By pretraining a trajectory diffusion model on large-scale motion datasets and fine-tuning on high-quality humanoid data, the system learns versatile, text-controllable motion generation.
The resulting controller enables general-purpose, physically grounded humanoid behaviors from textual prompts such as “turn left” or “reach forward.”
This project explores the integration of natural language understanding with optimal control and reinforcement learning for expressive humanoid motion generation.
\end{abstract}

%% Main sections
\section{Introduction}
State of the art humanoid whole-body control trained end-to-end in simulations have achieved remarkable agility and expressiveness 
\cite{liao2025beyondmimicmotiontrackingversatile, ji2024exbody2, cheng2024express, zhang2025falcon, li2025amo}.
However, most existing systems are restricted to single-skill policies and lack the ability to generalize across diverse motions or respond to high-level semantic commands.

This project aims to develop a unified, language-conditioned whole-body control policy capable of producing dynamically stable motion trajectories from text prompts. 
The controller is based on a trajectory diffusion model pre-trained on motion datasets with and without language labels, followed by fine-tuning on high-quality, re-targeted humanoid control data.

Our long-term vision is a humanoid platform that can perform a broad range of physically grounded tasks upon natural language instruction, enabling more intuitive human–robot interaction.

%!TEX root = ../main.tex
\section{Related Work}
\label{sec:related-work}

Recent humanoid control frameworks \cite{liao2025beyondmimicmotiontrackingversatile} \cite{ji2024exbody2} have demonstrated progress in motion imitation and task-specific control. However, these approaches are limited to single-behavior policies. In addition, language-conditioned control has been explored in manipulation and locomotion tasks \cite{DBLP:journals/corr/abs-2109-01115}, but its application to whole-body humanoid control remains underdeveloped.

Recent work \cite{shao2025langwbclanguagedirectedhumanoidwholebody} have introduced an observation-conditioned, reactive whole-body controller that translates natural language instructions into humanoid motion. Their approach employs a teacher-student RL framework to distill into a compact, language-conditioned policy. While recent works demonstrate impressive real-world performance, policy distillation paradigms inherently restrict behavior to the manifold of trajectories demonstrated by the teacher, limiting the capacity for novel, compositional motion generation.

Our proposed method treats the problem as a language-guided trajectory modeling problem, where the policy corresponds to a particular masking that in-paints the actions. This probabilistic formulation naturally captures the diversity of behaviors corresponding to a single command (e.g., “turn left” → slow turn, fast pivot, or sidestep) and enables zero-shot novelty through the recombination of learned motion primitives. We want to develop a general framework where rich, unlabeled data mixture can be used to produce strong robot priors, where the policy, inverse model, and the forward models are simply variants among the joint and conditional distributions across the language, perception and actions modalities.

%!TEX root = ../main.tex
\section{Problem Formulation}
\label{sec:formulation}

We formulate humanoid control as a partially observable Markov decision process (POMDP):

\[
\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{O}, P, R, \gamma)
\]

\begin{itemize}
    \item \textbf{State space} $\mathcal{S}$: joint angles, velocities, base pose, and contact flags.
    \item \textbf{Observation space} $\mathcal{O}$: proprioceptive sensors.
    \item \textbf{Action space} $\mathcal{A}$: joint torques or target positions.
    \item \textbf{Transition} $P$: deterministic simulator dynamics (MuJoCo).
    \item \textbf{Reward} $R$: combines task completion, stability (ZMP margin), smoothness, and imitation terms.
    \item \textbf{Language input:} text embedding $z_{\text{text}}$ serves as a context variable conditioning the policy or diffusion process.
\end{itemize}

\subsection{Assumptions}
\begin{itemize}
    \item Dynamics are fully known through simulation.
    \item No perception or real-world noise is considered.
    \item Language corresponds to a discrete set of semantic goals (e.g., ``step forward,’’ ``turn left’’).
\end{itemize}

\subsection{Data Sources}
\begin{itemize}
    \item AMASS \cite{AMASS:ICCV:2019} dataset with motion and natural language annotations.
    \item LAFAN1 \cite{harvey2020robust} (retargeted for Unitree G1).
\end{itemize}

\subsection{Infrastructure}
Training is conducted using MJLab (MuJoCo + RSL) for simulation, PyTorch for model development, and HuggingFace tokenizers for text embeddings.

\subsection{Relevance to the Course}

This project directly aligns with the study of sequential decision making and control in dynamic, high-dimensional systems central to optimal control (OC) and reinforcement learning (RL).

\begin{itemize}
    \item \textbf{Decision-maker:} learned policy $\pi_\theta(a_t \mid s_t, \text{text})$ representing the humanoid’s language-conditioned controller.
    \item \textbf{Dynamics:} continuous nonlinear system $s_{t+1} = f(s_t, a_t)$ modeled in MuJoCo.
    \item \textbf{Sequential aspect:} requires long-horizon trajectory optimization for balance and task completion.
    \item \textbf{Connection to OC/RL:}
    \begin{itemize}
        \item Incorporates imitation and RL rollouts for pretraining.
        \item Explores diffusion-based trajectory optimization analogous to stochastic optimal control.
        \item Evaluates policies under cumulative reward including task success, smoothness, and energy efficiency.
    \end{itemize}
\end{itemize}

This work bridges language grounding, imitation learning, and diffusion-based optimal control, aiming for versatile and interpretable humanoid motion generation.

%!TEX root = ../main.tex
\section{Proposed Method}
\label{sec:method}

\subsection{Overview}

We propose a language-conditioned trajectory diffusion model for whole-body humanoid control. 
The pipeline consists of three stages:

\begin{enumerate}
    \item \textbf{Pretraining:} Train an unconditional diffusion model $p_\theta(\tau)$ over humanoid motion trajectories.
    \item \textbf{Conditioning:} Introduce text conditioning $p_\theta(\tau \mid z_{\text{text}})$ using cross-attention or FiLM layers.
    \item \textbf{Control Integration:} Deploy generated trajectories into a PD controller for stable execution
\end{enumerate}

This hybrid approach combines imitation learning with policy optimization for robust, semantically aligned control.

\subsection{System Architecture}

\begin{itemize}
    \item \textbf{Inputs:} text prompt and proprioceptive history.
    \item \textbf{Outputs:} time-parameterized per-joint control trajectories.
    \item \textbf{Backbone:} shared temporal encoder with decoders for motion and language.
    \item \textbf{Losses:} diffusion loss, imitation loss, and alignment loss between text and motion.
\end{itemize}

\subsection{Data and Tokenization}

We utilize BeyondMimic \cite{liao2025beyondmimicmotiontrackingversatile} and LAFAN1 \cite{harvey2020robust} datasets for pretraining motion representations. 
A spatiotemporal tokenizer aligns motion frames with textual segments, and time normalization is applied to enhance temporal generalization.

\subsection{Evaluation Plan}

\textbf{Baselines:}
\begin{itemize}
    \item BeyondMimic RL (task-specific policy)
    \item ExBody2 imitation controller
    \item Diffusion model without language conditioning
\end{itemize}

\textbf{Metrics:}
\begin{itemize}
    \item Task success rate (prompt compliance)
    \item Motion realism (FID in latent space)
    \item Trajectory smoothness and stability
    \item Language–motion alignment (CLIP similarity or human evaluation)
\end{itemize}

\textbf{Expected Outcomes:}
\begin{itemize}
    \item Demonstrate feasibility of language-conditioned humanoid control.
    \item Release a dataset of captioned humanoid trajectories.
    \item Provide an open-source simulation and training pipeline.
\end{itemize}

%!TEX root = ../main.tex
\section{Experiments}
\label{sec:experiments}

\subsection{Scope and Evaluation}

The project’s scope is limited to simulated humanoid control without perception.
Experiments evaluate how well language-conditioned policies generalize to unseen text prompts within predefined skill categories such as reaching, stepping, turning, grasping, pushing, and balancing.

We evaluate on the following axes:
\begin{itemize}
    \item \textbf{Physical validity:} trajectory stability and balance.
    \item \textbf{Task execution:} correct motion given text input.
    \item \textbf{Smoothness:} motion continuity and energy efficiency.
\end{itemize}

\subsection{Infrastructure}

Experiments are run using the MJLab environment, RSLgym, and PyTorch.
Model training leverages diffusion-based motion generation conditioned on text embeddings from pretrained language models.
% \input{sections/conclusion.tex}

\clearpage
%% Appendix
% \begin{center}
%     {\Large\bf Appendix}
% \end{center}
% \appendix
% \input{sections/app-proof-convergence.tex}

%% References
\printbibliography
% \bibliographystyle{plain}
% \bibliography{refs}

\end{document}